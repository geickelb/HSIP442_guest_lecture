{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div>\n",
    "<img src=\"../images/joke.png\"\n",
    "     height=500\n",
    "     width= 500>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### an important note before we start:\n",
    "\n",
    "<img src=\"../images/model_comparison.png\" \n",
    "     height=500\n",
    "     width= 500\n",
    "     alt=\"Example Visualization of a Snapshot (aggregated) Prediction Model\" \n",
    "     title=\"Snapshot Variable Prediction Model\" />\n",
    "    \n",
    "    \n",
    "sometimes a fancy algorithm can make a big impact, but often the difference between a well tuned simple and complex algorithm is not that high.\n",
    "\n",
    "Fancy algorithms don't magically make perfect predictions. The legwork done before and after model building is often the most important\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Now, lets learn about fancy algorithms: Random Forest and Gradient Boosted Trees\n",
    "* necessary background:\n",
    "    * CART trees\n",
    "    * bagging\n",
    "    * ensembling\n",
    "    * gradient boosting\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification And Regression Trees (CART): glorified if/then statements\n",
    "### example tree:\n",
    "<img src=\"../images/Example_Decision_Tree.png\" \n",
    "     height=500\n",
    "     width= 500\n",
    "     alt=\"Example Visualization of a Snapshot (aggregated) Prediction Model\" \n",
    "     title=\"Snapshot Variable Prediction Model\" />\n",
    "     \n",
    "###  written as a rulebased classifier:\n",
    "1. If Height > 180 cm Then Male\n",
    "1. If Height <= 180 cm AND Weight > 80 kg Then Male\n",
    "1. If Height <= 180 cm AND Weight <= 80 kg Then Female\n",
    "1. Make Predictions With CART Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* A final fitted CART model divides the predictor (x) space by successively splitting into rectangular regions and models the response (Y) as constant over each region\n",
    "* can be schematically represented as a \"tree\":\n",
    "    * each interior node of the tree indicates on which predictor variable you split and where you split\n",
    "    * each terminal node (aka leaf) represents one region and indicates the value of the predicted response in that region\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CART Math: for those who want to take a simple idea and make it confusing\n",
    "\n",
    "we can write the equation of a regression tree as: $Y = g(X, \\theta) + \\epsilon$ \n",
    "\n",
    "where: <br> $g(X;\\theta)= \\sum^M_{m=1}I(x \\in R_m)$\n",
    "\n",
    "\n",
    "* $M$ = total number of regions (terminal nodes)\n",
    "* $R_m$ = $m$th region\n",
    "* $I(x \\in R_m)$ = indicator function = $\\{ \\begin{array}{lr} 1:x \\in R_m \\\\ 0:x \\notin R_m \\end{array} $\n",
    "* $c_m$ =constant predictor over Rm \n",
    "* $\\theta$ = all parameters and structure (M, splits in Rm’s, cm’s, etc)\n",
    "\n",
    "\n",
    "#### illustration of tree for $M=6$ regions, $k=2$ predictors, and $n=21$ training observations\n",
    "<img src=\"../images/CART3.png\" \n",
    "     height=500\n",
    "     width= 500\n",
    "     alt=\"Example Visualization of a Snapshot (aggregated) Prediction Model\" \n",
    "     title=\"Snapshot Variable Prediction Model\" />\n",
    "\n",
    "where:\n",
    "* R is the region number\n",
    "* N = the number of data points in that region\n",
    "* $\\hat y$ is the predicted response for that region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### in more simple terms: a CART tree defines regions of the predictor space to correspond to a predicted outcome value \n",
    "* when fitting a CART tree, the model grows one tree node at a time\n",
    "* at each split, the tree defines boundaries in predictor space based on what REDUCES THE TRAINING ERROR THE MOST\n",
    "* stops making splits when the reduction in error falls below a threshold\n",
    "* branches can be pruned (ie nodes/boundaries removed)to reduce overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**example**: $GPA = g((HSrank, ACTscore), \\theta) + \\epsilon$ \n",
    "\n",
    "<img src=\"../images/CART2.png\" \n",
    "     height=800\n",
    "     width= 800\n",
    "     alt=\"Example Visualization of a Snapshot (aggregated) Prediction Model\" \n",
    "     title=\"Snapshot Variable Prediction Model\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why use a CART?\n",
    "* easy to interpret\n",
    "* handle categorical variables intuitively\n",
    "* computationally efficient\n",
    "* have reasonable predictive performance\n",
    "* not sensitive to MONOTONIC transformations (ie anything that preserves the order of a set, like log scaling). \n",
    "* form the basis for many commonly used algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "--------\n",
    "# More Background: Ensembling, Bootstrapping & Bagging \n",
    "\n",
    "* **Ensemble** (in machine learning) :\n",
    "    * use multiple learning algorithms to obtain better predictive performance than a single learning algorithm alone.\n",
    "    * concrete finite set of alternative models\n",
    "    * but allows for much more flexible structure to exist among those\n",
    "\n",
    "\n",
    "* **Bootstrapping**: \n",
    "    *  ~sampling WITH replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Bagging**: (bootstrapping and aggregating)\n",
    "    * a type of ensembling\n",
    "    * designed to improve  stability & accuracy of some ML algorithms\n",
    "    * algorithm: \n",
    "        1. bootstrap many different sets from your training data \n",
    "        1. fit a model to each\n",
    "        1. average the predicted output (for regression) or voting (for classification) from bootstraped models across x values.\n",
    "  \n",
    "\n",
    "**example**: \n",
    "* for $b= 1, 2, ..., B$: (where B= number of times we bootstrap, and b = current iteration)\n",
    "    * generate bootstrap sample of size n\n",
    "    * fit model (any kind) $g(x;\\hat\\theta^b)$\n",
    "    * repeat for specified # of bootstraps\n",
    "    * take y at each value of x as the average response of each of the boostrapped models: $\\hat y(x) = \\frac{1}{B}\\Sigma^B_{b=1}g(x;\\hat\\theta^b)$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Visualizations**: \n",
    "visualization for bagging ensemble (source: KDnuggets)\n",
    " \n",
    "  <img src=\"../images/bagged_ensemble.jpg\" \n",
    " height=500\n",
    " width= 400\n",
    " alt=\"source KDNuggets\" \n",
    " title=\"Snapshot Variable Prediction Model\" />\n",
    "  \n",
    "\n",
    "plotting boostrapped and bagged models: (source: Wikipedia)\n",
    " \n",
    "  <img src=\"../images/bagging_models.png\" \n",
    " height=300\n",
    " width= 300\n",
    " alt=\"source: wikipedia\" \n",
    " title=\"Snapshot Variable Prediction Model\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### when is bagging useful:\n",
    "* For predictors where fitting is unstable (i.e., a small change in the data gives a very different fitted model) and/or the structure is such that the sum of multiple predictors no longer has the same structure\n",
    "\n",
    "### when does bagging have no effect:\n",
    "* For predictors that are linear ($\\hat y$ a linear function of training $y$)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-----\n",
    "# Random Forest: leveraging the wisdom of crowds\n",
    "\n",
    "* general idea: grow a bunch of different CART trees and let them all vote to get the prediction\n",
    "\n",
    "* Algorithm detail: \n",
    "    1. draw a bootstrap sample $Z^*$ of size $N$ from the training data\n",
    "    1. grow a CART tree $T_b$ to the bootstrapped data by recursively repeating the following steps for each terminal node until the  minimum node size $n_min$ is reached:\n",
    "        1. randomly select $m$ predictor variables\n",
    "        1. pick the best variable/spit-point (aka boundary) for the $m$ predictor variables\n",
    "        1. split the node into two daughter nodes\n",
    "    1. output the ensemble of trees ${T_b}^B_1$.\n",
    "    \n",
    "    * make a prediction by taking majority vote (classification) or averaging prediction from each tree (regression)\n",
    "\n",
    "* in more simple terms: grow and train a lot of CART trees with a maximum size, each using randomly sampled observations (with replacement) and predictor variables (without replacement)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Random forest simplified (source: towards data science blog)\n",
    "\n",
    "<img src=\"../images/rf_vis.png\" \n",
    " height=500\n",
    " width= 500\n",
    " alt=\"source: wikipedia\" \n",
    " title=\"Snapshot Variable Prediction Model\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-----\n",
    "# Gradient boosting: leveraging the stupidity of crowds \n",
    "        \n",
    "* **Boosting**: \n",
    "    * a type of ensembling that turns a set of weak learners (ie predictors that are slightly better than chance) into a strong learner\n",
    "    * many different types of algorithms that achieve boosting \n",
    "       \n",
    "* **Gradient Boosting**  :\n",
    "    * Like other boosting methods, gradient boosting combines weak \"learners\" into a single strong learner in an iterative fashion.\n",
    "stated two different ways:\n",
    "    * ensembles simple/weak CART trees in a stage-wise fashion and generalizes them by allowing optimization of an arbitrary differentiable loss function.\n",
    "    * boosting sequentially fits simple/weak CART trees to the residuals from the previous iteration, taking the final model to be the sum of the individual models from each iteration\n",
    "\n",
    "\n",
    "explaining in the least-square regression setting:\n",
    "* goal: \"teach\" a model $F$ to predict values of the form $\\hat y=F(x)$ by minimizing the mean squared error $\\frac{1}{n}\\sum_i (\\hat y_i - y_i)^2$, where i indexes over some training set of size n. \n",
    "* at each iteration $m$, $1\\leq m \\leq M$, it may be assumed that there is some imperfect model $F_m$ (usually starts with just mean y). \n",
    "* in each iteration, the algorithm improves on $F_m$ by constructing a new model that adds an estimator $h$ to make it better: $F_{m+1}(x)= F_m(x) + h(x)$\n",
    "* a perfect $h$ implies that $F_{m+1}(x)= F_m(x) + h(x)=y$ or $ h(x) = y - F_m(x)$\n",
    "* thus, gradient boosting will fit $h$ to the **residual** $y-F_m(x)$. \n",
    "* in each iteration, $F_{m+1}$ attemps to correct the errors of it's predecessor $F_m$.\n",
    "\n",
    "to generalize this, we can observe that residuals $y- F(x)$ for a given model are the **negative gradients** (with respect to $F(x)$) of the squared error loss function $\\frac{1}{2}(y-F(x))^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "for those of you who want the maths:\n",
    "\n",
    "<img src=\"../images/gbm_algorithm.png\" \n",
    " height=800\n",
    " width= 800\n",
    " alt=\"source: wikipedia\" \n",
    " title=\"Snapshot Variable Prediction Model\" />\n",
    " \n",
    "<br>\n",
    "\n",
    "for those of you who want pictures:\n",
    "\n",
    "<img src=\"../images/gbm_vis.png\" \n",
    " height=800\n",
    " width= 800\n",
    " alt=\"source: wikipedia\" \n",
    " title=\"Snapshot Variable Prediction Model\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-----\n",
    "# final thoughts about RandomForest and GBM\n",
    "\n",
    "* overfitting is definitely a thing with these models, so understanding some parameters is important.\n",
    "\n",
    "\n",
    "### RF\n",
    "* tree size (depth) = big deal, deeper trees = more likely to overfit\n",
    "* more trees = not that big of a deal. they make the out of bag error plot looks smoother\n",
    "\n",
    "### GBM\n",
    "* tree size isn't that big of a deal, (smaller trees mean you can still capture error in next tree)\n",
    "* more trees = more likely to overfit. too many trees = the out of bag error look more U shaped. \n",
    "\n",
    "### both algorithms:\n",
    "* neither alrogithm handles heavily imballanced classes very well (this can be an entire lecture on it's own)\n",
    "* both inherit all of the benefits of regular CART trees\n",
    "* both are better at regression than CART trees\n",
    "* both handle much more complex non-linear relationships between predictor and responce\n",
    "* both are capable of capturing **SOME** higher order predictor interactions, but these are often masked by marginal effects and cannot be differentiated from them. (ongoing research into this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
